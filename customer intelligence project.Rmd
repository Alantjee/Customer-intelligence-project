---
title: "Customer Intelligence and Big Data"
author: "Alan Rijnders and Lorenzo Severi"
date: "11/4/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#here I set the working directory for my personal computer
knitr::opts_knit$set(root.dir = "C:/Documents/Customer-intelligence-project")


```

We start reading in the data to perform the analysis
```{r}
#read in data
data <- read.csv("ch.csv")
library(dplyr)
library(ggplot2)
#install.packages('caret', dependencies = TRUE)
#install with dependencies = TRUE is important for the calculation of sensitivity and specficity
library(caret)
```


We now start exploring the dataset
```{r}
#summary of the dataset
summary(data)
```


```{r }
head(data, 5)
```

We are interested to see how many customers have remained at the company and how many have left. 
```{r }
table(data$Attrition_Flag)
```
In other words, out of total 10127 customers in the database we have 8500 customers that have remained at the company ,whereas 1627 customers have left. 

```{r }
table(data$Gender)
```

```{r }
table(data$Dependent_count)
```

```{r , echo=FALSE}
sapply(data, class)
```

Since Attrition_Flag is a character variable with two possible values: either "Existing Customer" or "Attrited Customer"  for modeling purposes we recode this variable into a factor with levels 0 and 1, where 1 represents a customer that has left the company when the person is still a customer at the company. Similarly we transform the variables Gender, Education Level, Marital Status, Income Category and Card Category to factor variables. 

```{r}


data <- transform(
  data, 
  Attrition_Flag = as.factor(Attrition_Flag),
  Gender = as.factor(Gender),
  Education_Level = as.factor(Education_Level),
  Marital_Status = as.factor(Marital_Status),
  Income_Category = as.factor(Income_Category),
  Card_Category = as.factor(Card_Category))
```

We now split the data into a training and a test sample, we use the training sample to train our model and the test sample to test our predictions to assess the power of our models. 
```{r,results='hide'}
smp_size <- floor(0.75 * nrow(data))

## set the seed to make your partition reproducible
set.seed(12345)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train <- data[train_ind, ]
test <- data[-train_ind, ]
```



```{r,results='hide'}
library(rpart)
tree <- rpart(Attrition_Flag ~ . - X , method = "class", data = train)
printcp(tree)
plotcp(tree)
summary(tree)

```


```{r }
# plot tree
plot(tree, uniform=TRUE,
   main="Classification Tree for Attrition")
text(tree, use.n=TRUE, all=TRUE, cex=.8)

```

```{r }
#library caret is a comprehensive library support all sorts of model analysis
library(caret)
options(digits=4)
# assess the model's accuracy with train dataset by make a prediction on the train data. 
Predict_model1_train <- predict(tree, train, type = "class")
#build a confusion matrix to make comparison
conMat <- confusionMatrix(as.factor(Predict_model1_train), as.factor(train$Attrition_Flag))
#show confusion matrix 
conMat$table
```

```{r }

sensitivity(conMat$table)
specificity(conMat$table)
print(accuracy <- (6190+911)/(6190+911+291+203))
```
The model looks to do a decent job, our sensitivity seems to be quite higher than our specificity, which implies that our model is better at correctly classifying clients that left than at finding true loyal customers. This could be because of ...

Now that we have constructed the model we proceed by predicting the values in the test set in order to assess the suitability of the model. 

```{r}
Predict_model1_test <- predict(tree, test, type = "class")

conMattest <- confusionMatrix(as.factor(Predict_model1_test), as.factor(test$Attrition_Flag))

conMattest$table
```

```{r }

sensitivity(conMattest$table)
specificity(conMattest$table)
print(accuracy <- (2032+315)/(2032+315+110+75))
```
There is not much difference between the accuracy for our model when comparing for the test and training set. The Sensitivity is slightly higher(0.01) and the specificity slightly lower(0.01). The accuracy is slightly lower than when predicting on the training set, however the difference is marginal

A 2nd type of model that we could implement is a random forest. CART decision trees are easily interpretable but output can be ... because of ...      Therefore we implement a random forest model that uses a bagging procedure producing ... regression trees and takes the average of each regression tree to improve the .. of the model results. 

```{r }
library(randomForest)

rf <- randomForest(Attrition_Flag ~ . - X , data = train)

summary(rf)
```